{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da6e30d6-6641-4c91-9952-18b569c4363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn, einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778fc2c-4034-49c5-8e20-855a2595b5cd",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eedafc-415a-450a-b962-367b4821e2ca",
   "metadata": {},
   "source": [
    "## Patch Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2ec11-e26e-4ef3-ab23-bfd98df64bff",
   "metadata": {},
   "source": [
    "If I have an image `(3, 16, 16)`, and I designate patch size to be `(3, 4, 4)`. Then I should have 16 patches and each patch contains `3*4*4=48` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60e6329-02b4-4ef2-9ae8-cecfbb1c3d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image torch.Size([1, 3, 16, 16])\n",
      "Unfolded torch.Size([1, 48, 16])\n",
      "View as image patches torch.Size([1, 48, 4, 4])\n",
      "Move patch values to last axis torch.Size([1, 4, 4, 48])\n",
      "Final output torch.Size([1, 4, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 4\n",
    "unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "linear = nn.Linear(3 * patch_size ** 2, 32)\n",
    "\n",
    "x = torch.rand(1, 3, 16, 16)\n",
    "print('Image', x.shape)\n",
    "x = unfold(x)\n",
    "print('Unfolded', x.shape)\n",
    "x = x.view(1, -1, patch_size, patch_size)\n",
    "print('View as image patches', x.shape)\n",
    "x = x.permute(0, 2, 3, 1)\n",
    "print('Move patch values to last axis', x.shape)\n",
    "y = linear(x)\n",
    "print('Final output', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5ac4d-5565-41a5-a988-8534fb89272d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functional Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7129feb-268d-484f-a228-0ba19ddf1231",
   "metadata": {},
   "source": [
    "There are some interesting functional approaches to structure resdiual and layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae142cf-4b6c-4688-8c7d-4a951ea2e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac6685-db26-4d68-b163-d9d4668c4cbe",
   "metadata": {},
   "source": [
    "Now I can compose residual with any other inner block. This is pretty neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95796968-e616-491d-9e11-ecd203f90887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck = nn.Sequential(\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 128),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "res_block = Residual(bottleneck)\n",
    "\n",
    "x = torch.rand(1, 4, 4, 128)\n",
    "y = res_block(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50239f-b416-4a43-812c-b076d0aee837",
   "metadata": {},
   "source": [
    "Extend the same functional concept to `LayerNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2527f10b-c97b-4e3d-99c6-524694442d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c732b882-71b7-4c6d-86fa-2881f89ce94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck = nn.Sequential(\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 128),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "norm_block = PreNorm(128, bottleneck)\n",
    "\n",
    "x = torch.rand(1, 4, 4, 128)\n",
    "y = norm_block(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21c117-9f70-4684-8789-54a000364e67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Relative Position Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d2ccb-0524-408c-9674-08d32b60bb42",
   "metadata": {},
   "source": [
    "Each window contains `(M, M)` patches. The `M` is the window size. Now each patch needs to learn a relative position embedding. `M**2` is the number of patches in each window.\n",
    "\n",
    "If we don't use relative position, then the position embedding is a matrix of `(M**2, M**2)`. It's every position to every position.\n",
    "\n",
    "If we use relative position, then relative position along each axis lies in the range of `[-M + 1, M - 1]`, i.e. if `M = 4`, then we have `[-3, -2, -1, 0, 1, 2, 3]` for each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586e4f73-1737-4b26-acbe-ffabb1548da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [0, 3],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2],\n",
      "        [1, 3],\n",
      "        [2, 0],\n",
      "        [2, 1],\n",
      "        [2, 2],\n",
      "        [2, 3],\n",
      "        [3, 0],\n",
      "        [3, 1],\n",
      "        [3, 2],\n",
      "        [3, 3]])\n",
      "torch.Size([16, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "window_size = 4\n",
    "indices = np.array([[x, y] for x in range(window_size) for y in range(window_size)])\n",
    "indices = torch.tensor(indices)\n",
    "print(indices.shape)\n",
    "print(indices)\n",
    "distances = indices[None, :, :] - indices[:, None, :]\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f90c3c-4a2b-45d3-abd2-2b42bd1be473",
   "metadata": {},
   "source": [
    "We have 16 to 16 positions, the distances cache the offset between `positions[i]` to `positions[j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb79c6a4-9e80-43a7-89e1-cebd42d307c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, -1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4e88cb-fe09-457d-8cf1-fce4f9913ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28818147-c2e3-408c-993c-5c00c03e716b",
   "metadata": {},
   "source": [
    "This will return all the `i` offsets for all 16 positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1f4f12-38dc-4eec-b781-ffaee7344a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3],\n",
       "        [ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3],\n",
       "        [ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3],\n",
       "        [ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3],\n",
       "        [-1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2],\n",
       "        [-1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2],\n",
       "        [-1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2],\n",
       "        [-1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2],\n",
       "        [-2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1],\n",
       "        [-2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1],\n",
       "        [-2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1],\n",
       "        [-2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0,  1,  1,  1,  1],\n",
       "        [-3, -3, -3, -3, -2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0],\n",
       "        [-3, -3, -3, -3, -2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0],\n",
       "        [-3, -3, -3, -3, -2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0],\n",
       "        [-3, -3, -3, -3, -2, -2, -2, -2, -1, -1, -1, -1,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae561745-8d74-4df9-98c5-05786b1f65ba",
   "metadata": {},
   "source": [
    "This will return all the `j` offsets for all 16 positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad466fe1-a188-4009-b0a3-0ba1043112d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3],\n",
       "        [-1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2],\n",
       "        [-2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1],\n",
       "        [-3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0],\n",
       "        [ 0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3],\n",
       "        [-1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2],\n",
       "        [-2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1],\n",
       "        [-3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0],\n",
       "        [ 0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3],\n",
       "        [-1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2],\n",
       "        [-2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1],\n",
       "        [-3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0],\n",
       "        [ 0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3],\n",
       "        [-1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2, -1,  0,  1,  2],\n",
       "        [-2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1, -2, -1,  0,  1],\n",
       "        [-3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0, -3, -2, -1,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b653306d-295d-4aad-941b-a7928e2aff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4e6e9c-0c05-4840-9d1b-fb627260e617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding[distances[:, :, 0], distances[:, :, 1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a60a25e-65ef-4f46-9fb3-e0e89adf56e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8387,  1.0895,  0.3368,  0.2537, -1.8590,  1.5533, -1.0229,  0.9228,\n",
       "         -1.3652, -3.2351,  0.1753,  0.3024, -0.6617, -0.2192,  0.3134,  1.2471],\n",
       "        [ 0.0322,  0.8387,  1.0895,  0.3368,  0.3960, -1.8590,  1.5533, -1.0229,\n",
       "          0.0525, -1.3652, -3.2351,  0.1753,  1.7376, -0.6617, -0.2192,  0.3134],\n",
       "        [-0.1150,  0.0322,  0.8387,  1.0895,  1.0195,  0.3960, -1.8590,  1.5533,\n",
       "          0.8233,  0.0525, -1.3652, -3.2351,  0.6204,  1.7376, -0.6617, -0.2192],\n",
       "        [-0.0605, -0.1150,  0.0322,  0.8387, -0.8598,  1.0195,  0.3960, -1.8590,\n",
       "          1.9468,  0.8233,  0.0525, -1.3652, -0.3867,  0.6204,  1.7376, -0.6617],\n",
       "        [ 1.1581, -0.2017,  1.1784,  1.4307,  0.8387,  1.0895,  0.3368,  0.2537,\n",
       "         -1.8590,  1.5533, -1.0229,  0.9228, -1.3652, -3.2351,  0.1753,  0.3024],\n",
       "        [-1.0957,  1.1581, -0.2017,  1.1784,  0.0322,  0.8387,  1.0895,  0.3368,\n",
       "          0.3960, -1.8590,  1.5533, -1.0229,  0.0525, -1.3652, -3.2351,  0.1753],\n",
       "        [ 1.0194, -1.0957,  1.1581, -0.2017, -0.1150,  0.0322,  0.8387,  1.0895,\n",
       "          1.0195,  0.3960, -1.8590,  1.5533,  0.8233,  0.0525, -1.3652, -3.2351],\n",
       "        [ 1.2998,  1.0194, -1.0957,  1.1581, -0.0605, -0.1150,  0.0322,  0.8387,\n",
       "         -0.8598,  1.0195,  0.3960, -1.8590,  1.9468,  0.8233,  0.0525, -1.3652],\n",
       "        [ 0.8572, -1.6615, -1.2511,  0.6525,  1.1581, -0.2017,  1.1784,  1.4307,\n",
       "          0.8387,  1.0895,  0.3368,  0.2537, -1.8590,  1.5533, -1.0229,  0.9228],\n",
       "        [-0.0438,  0.8572, -1.6615, -1.2511, -1.0957,  1.1581, -0.2017,  1.1784,\n",
       "          0.0322,  0.8387,  1.0895,  0.3368,  0.3960, -1.8590,  1.5533, -1.0229],\n",
       "        [-0.2218, -0.0438,  0.8572, -1.6615,  1.0194, -1.0957,  1.1581, -0.2017,\n",
       "         -0.1150,  0.0322,  0.8387,  1.0895,  1.0195,  0.3960, -1.8590,  1.5533],\n",
       "        [ 0.8039, -0.2218, -0.0438,  0.8572,  1.2998,  1.0194, -1.0957,  1.1581,\n",
       "         -0.0605, -0.1150,  0.0322,  0.8387, -0.8598,  1.0195,  0.3960, -1.8590],\n",
       "        [-0.0920, -0.6861, -2.3222, -1.0189,  0.8572, -1.6615, -1.2511,  0.6525,\n",
       "          1.1581, -0.2017,  1.1784,  1.4307,  0.8387,  1.0895,  0.3368,  0.2537],\n",
       "        [-2.5352, -0.0920, -0.6861, -2.3222, -0.0438,  0.8572, -1.6615, -1.2511,\n",
       "         -1.0957,  1.1581, -0.2017,  1.1784,  0.0322,  0.8387,  1.0895,  0.3368],\n",
       "        [-0.9178, -2.5352, -0.0920, -0.6861, -0.2218, -0.0438,  0.8572, -1.6615,\n",
       "          1.0194, -1.0957,  1.1581, -0.2017, -0.1150,  0.0322,  0.8387,  1.0895],\n",
       "        [-1.5300, -0.9178, -2.5352, -0.0920,  0.8039, -0.2218, -0.0438,  0.8572,\n",
       "          1.2998,  1.0194, -1.0957,  1.1581, -0.0605, -0.1150,  0.0322,  0.8387]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding[distances[:, :, 0], distances[:, :, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d231b-2337-4d1a-9f32-d4ef7e2f8b61",
   "metadata": {},
   "source": [
    "Since we defined position embedding to be a parameter, these values will be learned and updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a263ea3-3b0a-4a25-ba03-ed48ccd89099",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cyclic Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041386c-02b6-4453-8d67-ed377d62452b",
   "metadata": {},
   "source": [
    "Suppose the input has 8 by 8 patches and each patch has embedding dimension 32, let's create 4 windows. When we apply cyclic shift, the displacement will shift the element by `(i, j)` amount. In the following example, the shift pushes every element by `(2, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2aec8f2-9398-4d17-94e6-56734884d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12., 13., 14., 15., 16.],\n",
      "        [17., 18., 19., 20., 21., 22., 23., 24.],\n",
      "        [25., 26., 27., 28., 29., 30., 31., 32.],\n",
      "        [33., 34., 35., 36., 37., 38., 39., 40.],\n",
      "        [41., 42., 43., 44., 45., 46., 47., 48.],\n",
      "        [49., 50., 51., 52., 53., 54., 55., 56.],\n",
      "        [57., 58., 59., 60., 61., 62., 63., 64.]])\n",
      "tensor([[55., 56., 49., 50., 51., 52., 53., 54.],\n",
      "        [63., 64., 57., 58., 59., 60., 61., 62.],\n",
      "        [ 7.,  8.,  1.,  2.,  3.,  4.,  5.,  6.],\n",
      "        [15., 16.,  9., 10., 11., 12., 13., 14.],\n",
      "        [23., 24., 17., 18., 19., 20., 21., 22.],\n",
      "        [31., 32., 25., 26., 27., 28., 29., 30.],\n",
      "        [39., 40., 33., 34., 35., 36., 37., 38.],\n",
      "        [47., 48., 41., 42., 43., 44., 45., 46.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 8, 8, 32)\n",
    "\n",
    "# Make it more readable, assign 1...64 to the first element of each embedding.\n",
    "x[:, :, :, 0] = torch.arange(1, 65).view(8, 8)\n",
    "\n",
    "window_size = 4\n",
    "displacement = window_size // 2\n",
    "print(x[0, :, :, 0])\n",
    "rolled_x = torch.roll(x, shifts=(displacement, displacement), dims=(1, 2))\n",
    "print(rolled_x[0, :, :, 0])\n",
    "\n",
    "rolled_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e8e69-d728-4f3f-862d-12c96558838b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Masking for Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbed9d-f02d-441b-a475-4483301b23f6",
   "metadata": {},
   "source": [
    "The input will be re-arrange into windows. Within each window, we have `(4, 4)` patches with window size 4. The mask is applied after position embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c06037e-b598-4864-b3ab-65ccc3e66afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 4\n",
    "displacement = window_size // 2\n",
    "\n",
    "upper_lower_mask = torch.zeros(window_size**2, window_size**2)\n",
    "print(upper_lower_mask.shape) # Same shape as relative position embedding.\n",
    "\n",
    "upper_lower_mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "upper_lower_mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "upper_lower_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d64a0880-eefb-40df-9c4f-65456f4c6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n",
      "torch.Size([4, 4, 4, 4])\n",
      "torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.],\n",
       "        [-inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0., -inf, -inf, 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 4\n",
    "displacement = window_size // 2\n",
    "\n",
    "left_right_mask = torch.zeros(window_size**2, window_size**2)\n",
    "print(left_right_mask.shape) # Same shape as relative position embedding.\n",
    "\n",
    "left_right_mask = rearrange(left_right_mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "print(left_right_mask.shape)\n",
    "left_right_mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "left_right_mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "left_right_mask = rearrange(left_right_mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "print(left_right_mask.shape)\n",
    "\n",
    "left_right_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bff10d-ded3-4420-80ab-3ebab722d717",
   "metadata": {},
   "source": [
    "## Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072405ea-d9af-4b52-82b0-39206847456b",
   "metadata": {},
   "source": [
    "After the window partition, my input is a batch tensor with patches and embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9040b4d-3acd-490b-9be8-594dc9e02aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "H = 16 # unit of patches\n",
    "W = 16 # unit of patches\n",
    "embed_dim = 64\n",
    "head_dim = 128\n",
    "num_heads = 8\n",
    "window_size = 4 # unit of patches\n",
    "\n",
    "x = torch.rand(B, H, W, embed_dim)\n",
    "to_qkv = nn.Linear(embed_dim, 3 * num_heads * head_dim)\n",
    "to_out = nn.Linear(num_heads * head_dim, embed_dim)\n",
    "\n",
    "qkv = to_qkv(x).chunk(3, dim=-1)\n",
    "num_win_h = H // window_size # Number of windows along height axis\n",
    "num_win_w = W // window_size # Number of windows along width axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a3767-2083-456f-a0fa-6b6ec261f93b",
   "metadata": {},
   "source": [
    "Each query, key, value tensor is of shape `(batch_size, num_heads, num_windows, num_patches, head_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6944f3c-2548-480c-9f5c-87f65c91b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q torch.Size([4, 8, 16, 16, 128])\n",
      "K torch.Size([4, 8, 16, 16, 128])\n",
      "V torch.Size([4, 8, 16, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "q, k, v = map(\n",
    "    lambda t: rearrange(t, 'b (num_win_h win_h) (num_win_w win_w) (h d) -> b h (num_win_h num_win_w) (win_h win_w) d',\n",
    "                        h=num_heads,\n",
    "                        win_h=window_size,\n",
    "                        win_w=window_size), qkv)\n",
    "\n",
    "print('Q', q.shape)\n",
    "print('K', k.shape)\n",
    "print('V', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f819231-b189-4899-a8c1-c0829d4035ac",
   "metadata": {},
   "source": [
    "The attention score will be computed with dot product of `Q` and `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be350ff4-b6cf-4a06-939d-910a4c4ece0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16, 16, 16])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dots = einsum('b h w i d, b h w j d -> b h w i j', q, k)\n",
    "dots.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6626f-44ee-4ed6-86e9-ee9ecd81136b",
   "metadata": {},
   "source": [
    "Positional embedding and masking will be added to the dot product and then perform softmax. I will skip it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "582effc3-2a87-4ca1-bc9b-b182e69c6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax torch.Size([4, 8, 16, 16, 16])\n",
      "Another matrix product torch.Size([4, 8, 16, 16, 128])\n",
      "Rearranged back to patch format torch.Size([4, 16, 16, 1024])\n"
     ]
    }
   ],
   "source": [
    "attn = dots.softmax(dim=-1)\n",
    "print('Softmax', attn.shape)\n",
    "\n",
    "out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "print('Another matrix product', out.shape)\n",
    "\n",
    "out = rearrange(out, 'b h (num_win_h num_win_w) (win_h win_w) d -> b (num_win_h win_h) (num_win_w win_w) (h d)',\n",
    "                h=num_heads,\n",
    "                win_h=window_size,\n",
    "                win_w=window_size,\n",
    "                num_win_h=num_win_h,\n",
    "                num_win_w=num_win_w)\n",
    "print('Rearranged back to patch format', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2d7fce4-860b-4e7c-8d64-45ef17bb66f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 16, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_out(out).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bd062-a815-485a-b0ec-2c6716e4e895",
   "metadata": {},
   "source": [
    "Since the input has 16 patches, the attention score is 16 to 16 self-attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
