{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d3754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7993ef3",
   "metadata": {},
   "source": [
    "# 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a431a2",
   "metadata": {},
   "source": [
    "## 1.1 Create Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f999d6",
   "metadata": {},
   "source": [
    "Tensors should stay on same GPU as I operate on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a118a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr = np.array([[1, 2], [3, 4]])\n",
    "tensor_a = torch.from_numpy(np_arr).to('cuda:0')\n",
    "tensor_b = torch.ones_like(tensor_a)\n",
    "tensor_c = torch.rand_like(tensor_b, dtype=torch.float)\n",
    "\n",
    "tensor_a.device, tensor_b.device, tensor_c.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f2114",
   "metadata": {},
   "source": [
    "By default, they should be on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cb6f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "tensor_a = torch.rand(shape)\n",
    "tensor_b = torch.ones(shape)\n",
    "tensor_c = torch.zeros(shape)\n",
    "\n",
    "tensor_a.device, tensor_b.device, tensor_c.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81430d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5070, 0.7800, 0.3710],\n",
       "        [0.6524, 0.0812, 0.5744]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee9707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.shape, tensor_a.dtype, tensor_a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777a9279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5070, 0.7800, 0.3710, 0.6524, 0.0812, 0.5744]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.view(-1, 6) # Same as reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14d48fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5070, 0.6524],\n",
       "        [0.7800, 0.0812],\n",
       "        [0.3710, 0.5744]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.permute(1, 0) # Permute like a tranpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d224b107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5070, 0.6524],\n",
       "        [0.7800, 0.0812],\n",
       "        [0.3710, 0.5744]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.T # Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c8ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5070, 0.7800, 0.3710, 0.6524, 0.0812, 0.5744],\n",
       "        [0.5070, 0.7800, 0.3710, 0.6524, 0.0812, 0.5744],\n",
       "        [0.5070, 0.7800, 0.3710, 0.6524, 0.0812, 0.5744]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.view(-1, 6).expand(3, 6) # Same as broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4af2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5070, 0.7800, 0.3710, 0.6524, 0.0812, 0.5744])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.view(-1, 6).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbfaf5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5070],\n",
       "        [0.7800],\n",
       "        [0.3710],\n",
       "        [0.6524],\n",
       "        [0.0812],\n",
       "        [0.5744]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a.view(6).unsqueeze(-1) # Flatten it and then unsqueeze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd1079f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 3],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_d = torch.tensor([1, 2, 3]).expand(2, 3).clone() # We must clone to allocate new memory.\n",
    "tensor_e = torch.tensor([2, 1])\n",
    "tensor_d[:, 0:2].add_(tensor_e) # In place operation\n",
    "tensor_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15f5b0",
   "metadata": {},
   "source": [
    "## 1.2 Tensor Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6957d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  7],\n",
       "        [14, 14]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 1], [2, 2]])\n",
    "tensor_b = torch.tensor([[3, 3], [4, 4]])\n",
    "tensor_a @ tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b355eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3],\n",
       "        [8, 8]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a * tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "460e81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4],\n",
       "        [6, 6]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a + tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc8023ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2, -2],\n",
       "        [-2, -2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a - tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43375c",
   "metadata": {},
   "source": [
    "## 1.3 Numpy & Tensor Share Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8492f969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a = torch.ones(5)\n",
    "np_a = tensor_a.numpy()\n",
    "tensor_a.add_(1)\n",
    "np_a # Numpy array is also modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2934926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_b = np.ones(5)\n",
    "tensor_b = torch.from_numpy(np_b)\n",
    "np.add(np_b, 1, out=np_b)\n",
    "tensor_b # Tensor is also modified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00d083",
   "metadata": {},
   "source": [
    "# 2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cb365",
   "metadata": {},
   "source": [
    "## 2.1 Auto Differentiaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a6257",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x_1, x_2) = \\sin(x_1) + \\cos(x_2)\n",
    "$$\n",
    "\n",
    "The derivative with respect to $x_1$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1} = \\cos(x_1)\n",
    "$$\n",
    "\n",
    "The derivative with respect to $x_2$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_2} = -\\sin(x_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e01084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.arange(0, torch.pi, 0.01, requires_grad=True)\n",
    "x2 = torch.arange(torch.pi, 2 * torch.pi, 0.01, requires_grad=True)\n",
    "f = torch.sin(x1) + torch.cos(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81a50951",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.ones_like(f)\n",
    "f.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba902c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(x1.grad == torch.cos(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55923cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(x2.grad == -1 * torch.sin(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87de1d8",
   "metadata": {},
   "source": [
    "## 2.2 Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd4b13",
   "metadata": {},
   "source": [
    "If I have a vector valued function,\n",
    "\n",
    "$$\n",
    "\\vec{y} = f(\\vec{x})\n",
    "$$\n",
    "\n",
    "then the gradient of $\\vec{y}$ with respect to the $\\vec{x}$ is a Jacobian matrix.\n",
    "\n",
    "$$\n",
    "\\def\\d{\\partial}\n",
    "\\frac{\\d \\vec{y}}{\\d \\vec{x}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\d y_1}{\\d x_1} & \\dots & \\frac{\\d y_1}{\\d x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\d y_m}{\\d x_1} & \\dots & \\frac{\\d y_m}{\\d x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f84298",
   "metadata": {},
   "source": [
    "Here's an example\n",
    "\n",
    "$$\n",
    "A \\vec{x} = \\vec{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee0566a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((3, 4), requires_grad=True)\n",
    "x = torch.rand((4, 1), requires_grad=True)\n",
    "y = A @ x\n",
    "y.retain_grad() # y is not a leaf node, thus I have to manually call retain grad.\n",
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2a6dc",
   "metadata": {},
   "source": [
    "`backward()` only works on scalar. The scalar I have is a summation of all elements of $\\vec{y}$.\n",
    "\n",
    "$$\n",
    "L = \\sum_i^m y_i\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{y}} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c97cd837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10be0f0",
   "metadata": {},
   "source": [
    "Also,\n",
    "\n",
    "$$\n",
    "\\def\\d{\\partial}\n",
    "\\frac{\\d L}{\\d \\vec{x}} = \\frac{\\d \\vec{y}}{\\d \\vec{x}}\\frac{\\d L}{\\d \\vec{y}} = \n",
    "J^\\intercal \\frac{\\d L}{\\d \\vec{y}} =\n",
    "A^\\intercal \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "166436b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(A.T @ y.grad == x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d14cd",
   "metadata": {},
   "source": [
    "In this sense, my Jacobian is actually just the $A$ matrix.\n",
    "\n",
    "$$\n",
    "\\def\\d{\\partial}\n",
    "A = J =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\d y_1}{\\d x_1} & \\dots  & \\dots & \\frac{\\d y_1}{\\d x_4}  \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\d y_3}{\\d x_1} & \\dots & \\dots & \\frac{\\d y_3}{\\d x_4}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961dd1d2",
   "metadata": {},
   "source": [
    "## 2.3 Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9fd760",
   "metadata": {},
   "source": [
    "A DAG is created when `backward()` is called. The node which I call `backward()` is the root node. I may be able to call multiple backward on multiple nodes. They are all considered roots. The input tensors or weight tensors are the leave nodes. In the example above, `A` is a leaf node, and `x` is also a leaf node. However, `y` is not a leaf node. It does not carry its own gradients unless I specify it to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ab419",
   "metadata": {},
   "source": [
    "## 2.4 Exclusion from DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b050d86",
   "metadata": {},
   "source": [
    "We can manually stop gradients from accumulating on certain leaf nodes. This is PyTorch's way to freeze parts of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b507bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 10) # Only train the head.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6306f",
   "metadata": {},
   "source": [
    "# 3. Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fc208",
   "metadata": {},
   "source": [
    "## 3.1 Define Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df20cfc",
   "metadata": {},
   "source": [
    "I will build a ResNet-9 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40de58",
   "metadata": {},
   "source": [
    "Note: Batch Normalization layers normalize the activations of the network so that they maintain a mean activation close to 0 and standard deviation close to 1. Because of this normalization, any bias added by a convolutional layer before a BatchNorm layer would be removed, making the bias unnecessary.\n",
    "\n",
    "https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c39c584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 24, 24])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        # First conv1 performs downsampling if stride > 1.\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_chans, momentum=0.9),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_chans, momentum=0.9),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # When we downsample, we also need to downsample the residual.\n",
    "        if stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_chans, momentum=0.9)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(residual)\n",
    "        x = x + residual\n",
    "        out = self.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "res_block = ResBlock(3, 128, stride=2)\n",
    "res_block(torch.rand((1, 3, 48, 48))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "67dcde69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 24, 24])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BottleneckResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, mid_chans, out_chans, stride=1):\n",
    "        super(BottleneckResBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, mid_chans, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(mid_chans, momentum=0.9),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Squeeze the inputs with mid_chans < in_chans to create a bottle neck.\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(mid_chans, mid_chans, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_chans, momentum=0.9),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(mid_chans, out_chans, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_chans, momentum=0.9),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_chans, momentum=0.9)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(residual)\n",
    "        x = x + residual\n",
    "        out = self.relu(x)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "res_block = BottleneckResBlock(256, 64, 128, stride=2)\n",
    "res_block(torch.rand((1, 256, 48, 48))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa6c3ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class ResNet9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.conv_stack_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64, momentum=0.9),\n",
    "            nn.ReLU(inplace=True), # We can save some memory here, since we don't worry about residuals here.\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=128, momentum=0.9),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.res_block_2 = ResBlock(128, 128)\n",
    "        self.conv_stack_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=256, momentum=0.9),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=256, momentum=0.9),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.res_block_4 = ResBlock(256, 256)\n",
    "        self.max_pool_5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(1024, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack_1(x)\n",
    "        x = self.res_block_2(x)\n",
    "        x = self.conv_stack_3(x)\n",
    "        x = self.res_block_4(x)\n",
    "        x = self.max_pool_5(x)\n",
    "        out = self.fc(x.view(-1, 1024))\n",
    "        return out\n",
    "\n",
    "\n",
    "net = ResNet9()\n",
    "out = net(torch.rand((2, 3, 32, 32)))\n",
    "net.zero_grad() # Reset gradients\n",
    "for param in net.parameters():\n",
    "    print(param.grad)\n",
    "\n",
    "out.backward(torch.rand_like(out))\n",
    "for param in net.parameters():\n",
    "    print(param.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070d1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
